environment = dev
kafka.default.bootstrap.servers = quickstart-bigdata:9092

# Impala
impala.db.name = 1357_disp_${environment}
impala.driver.className = com.cloudera.impala.jdbc.Driver
impala.jdbc.url = jdbc:impala://quickstart-bigdata:21050;AuthMech=0;

# Spark
spark.merger.fileSize.minimum.bytes = 67108864
spark.merger.logTable = t_file_merger_log
spark.merger.output.tmp.path = /user/osboxes/tmp/output/disp_spark_streaming
spark.merger.smallFile.maximum = 1
spark.savemode.append = Append
spark.streaming.jobs = ALL
spark.streaming.sleepTime = 30
spark.streaming.lifetime.amount = 12
spark.streaming.lifetime.unit = HOURS
spark.streaming.logTable = t_spark_ingestion_log

# Yarn
yarn.logs.ui.url = http://quickstart-bigdata:8088/cluster/app

################
##### JOBS #####
################

# Webdisp
webdisp.kafka.bootstrap.servers = ${kafka.default.bootstrap.servers}
webdisp.kafka.group.id = webdisp_${environment}
webdisp.kafka.topic.name = 1357-disp-webdisp-${environment}
webdisp.kafka.topic.partition = 0
webdisp.kafka.auto.offset.reset = earliest
webdisp.kafka.max.poll.records = 24
webdisp.datasource.dataClass = it.luca.disp.streaming.model.webdisp.WebdispPayload
webdisp.datasource.jobClass = it.luca.disp.streaming.app.job.WebdispJob
webdisp.datasource.consumerClass = it.luca.disp.streaming.app.consumer.WebdispConsumer
webdisp.spark.target.table = t_ingested_webdisp

# Jarvis
jarvis.kafka.bootstrap.servers = ${kafka.default.bootstrap.servers}
jarvis.kafka.group.id = jarvis_${environment}
jarvis.kafka.topic.name = 1357-disp-jarvis-${environment}
jarvis.kafka.topic.partition = 0
jarvis.kafka.auto.offset.reset = earliest
jarvis.kafka.max.poll.records = 24
jarvis.datasource.dataClass = it.luca.disp.streaming.model.jarvis.JarvisPayload
jarvis.datasource.jobClass = it.luca.disp.streaming.app.job.JarvisJob
jarvis.datasource.consumerClass = it.luca.disp.streaming.app.consumer.JarvisConsumer
jarvis.spark.target.table = t_ingested_jarvis

# Int002
int002.kafka.bootstrap.servers = ${kafka.default.bootstrap.servers}
int002.kafka.group.id = int002_${environment}
int002.kafka.topic.name = 1357-disp-int002-${environment}
int002.kafka.topic.partition = 0
int002.kafka.auto.offset.reset = earliest
int002.kafka.max.poll.records = 24
int002.datasource.dataClass = it.luca.disp.streaming.model.int002.Int002Payload
int002.datasource.jobClass = it.luca.disp.streaming.app.job.Int002Job
int002.datasource.consumerClass = it.luca.disp.streaming.app.consumer.Int002Consumer
int002.spark.target.table = t_ingested_int002

# Conduzione
conduzione.kafka.bootstrap.servers = ${kafka.default.bootstrap.servers}
conduzione.kafka.group.id = conduzione_${environment}
conduzione.kafka.topic.name = 1357-disp-conduzione-${environment}
conduzione.kafka.topic.partition = 0
conduzione.kafka.auto.offset.reset = earliest
conduzione.kafka.max.poll.records = 24
conduzione.datasource.dataClass = it.luca.disp.streaming.model.conduzione.ConduzionePayload
conduzione.datasource.jobClass = it.luca.disp.streaming.app.job.ConduzioneJob
conduzione.datasource.consumerClass = it.luca.disp.streaming.app.consumer.ConduzioneConsumer
conduzione.spark.target.table = t_ingested_conduzione